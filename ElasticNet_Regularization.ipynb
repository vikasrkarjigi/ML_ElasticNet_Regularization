{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW/GbECdh4T/BUUBga/p9W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vikasrkarjigi/ML_ElasticNet_Regularization/blob/main/ElasticNet_Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjkohtBUYxXf",
        "outputId": "2c23504e-9452-4d20-c5d8-85f4765e4d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Score (No Regularization) - Train: 0.8206194483296296, Test: 0.8666382520148099\n",
            "R2 Score (Lasso (L1)) - Train: 0.8278847156694062, Test: 0.8708716426348195\n",
            "R2 Score (Ridge (L2)) - Train: 0.8258692409649343, Test: 0.8678973202478635\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Ames Housing dataset\n",
        "data = pd.read_csv('/content/sample_data/AmesHousing.csv')\n",
        "\n",
        "# Drop irrelevant columns and handle missing data\n",
        "target = 'SalePrice'\n",
        "features = data.drop(['Order', 'PID', 'SalePrice'], axis=1)\n",
        "\n",
        "# Fill missing values with median for numeric columns, and mode for categorical columns\n",
        "features = features.fillna(features.median(numeric_only=True))\n",
        "features = features.fillna(features.mode().iloc[0])\n",
        "\n",
        "# Convert categorical columns to dummy variables (one-hot encoding)\n",
        "features = pd.get_dummies(features, drop_first=True)\n",
        "\n",
        "# Split the data into training and testing sets manually\n",
        "train_size = int(0.8 * len(features))\n",
        "X_train = features[:train_size]\n",
        "X_test = features[train_size:]\n",
        "y_train = data[target].values[:train_size]\n",
        "y_test = data[target].values[train_size:]\n",
        "\n",
        "# Standardize only numeric columns\n",
        "X_train_numeric = X_train.select_dtypes(include=[np.number])\n",
        "X_test_numeric = X_test.select_dtypes(include=[np.number])\n",
        "\n",
        "mean = X_train_numeric.mean(axis=0)\n",
        "std = X_train_numeric.std(axis=0)\n",
        "\n",
        "X_train_scaled = (X_train_numeric - mean) / std\n",
        "X_test_scaled = (X_test_numeric - mean) / std\n",
        "\n",
        "# Base class for regression models\n",
        "class BaseRegression:\n",
        "    def __init__(self):\n",
        "        self.theta = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        raise NotImplementedError(\"Fit method not implemented!\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        return X_b.dot(self.theta)\n",
        "\n",
        "    def r2_score(self, y_true, y_pred):\n",
        "        ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "        ss_residual = np.sum((y_true - y_pred) ** 2)\n",
        "        return 1 - (ss_residual / ss_total)\n",
        "\n",
        "# Linear Regression without Regularization\n",
        "class LinearRegressionNoReg(BaseRegression):\n",
        "    def fit(self, X, y):\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Adding bias term\n",
        "        self.theta = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
        "\n",
        "# Regularized Regression with L1 (Lasso) and L2 (Ridge)\n",
        "class RegularizedRegression(BaseRegression):\n",
        "    def __init__(self, alpha=0.01, l1_ratio=0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.l1_ratio = l1_ratio\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "        m, n = X_b.shape\n",
        "        self.theta = np.zeros(n)\n",
        "        for _ in range(1000):  # Fixed iterations\n",
        "            for j in range(n):\n",
        "                X_j = X_b[:, j]\n",
        "                residual = y - X_b.dot(self.theta) + self.theta[j] * X_j\n",
        "                rho = X_j.T.dot(residual)\n",
        "                if j == 0:  # Intercept\n",
        "                    self.theta[j] = rho / m\n",
        "                else:\n",
        "                    # Regularization terms\n",
        "                    l1_term = self.l1_ratio * self.alpha\n",
        "                    l2_term = (1 - self.l1_ratio) * self.alpha\n",
        "                    self.theta[j] = self._soft_threshold(rho / m, l1_term) / (1 + l2_term)\n",
        "\n",
        "    def _soft_threshold(self, rho, lambda_val):\n",
        "        return np.sign(rho) * max(0, abs(rho) - lambda_val)\n",
        "\n",
        "# ElasticNet Regression Class\n",
        "class ElasticNetRegression(RegularizedRegression):\n",
        "    def __init__(self, alpha=0.01, l1_ratio=0.5):\n",
        "        super().__init__(alpha, l1_ratio)\n",
        "\n",
        "# Function to perform grid search for ElasticNet\n",
        "def grid_search_elastic_net(X, y, alphas, l1_ratios):\n",
        "    best_score = -np.inf\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "    results = []  # To store scores for each alpha\n",
        "\n",
        "    for alpha in alphas:\n",
        "        for l1_ratio in l1_ratios:\n",
        "            model = ElasticNetRegression(alpha=alpha, l1_ratio=l1_ratio)\n",
        "            model.fit(X, y)\n",
        "            y_train_pred = model.predict(X)\n",
        "            y_test_pred = model.predict(X_test_scaled)\n",
        "            train_score = model.r2_score(y, y_train_pred)\n",
        "            test_score = model.r2_score(y_test, y_test_pred)\n",
        "\n",
        "            results.append((alpha, l1_ratio, train_score, test_score))\n",
        "\n",
        "            if test_score > best_score:\n",
        "                best_score = test_score\n",
        "                best_params = (alpha, l1_ratio)\n",
        "                best_model = model\n",
        "\n",
        "    return best_model, best_params, best_score, results\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"No Regularization\": LinearRegressionNoReg(),\n",
        "    \"Lasso (L1)\": RegularizedRegression(alpha=0.1, l1_ratio=1),\n",
        "    \"Ridge (L2)\": RegularizedRegression(alpha=0.1, l1_ratio=0),\n",
        "}\n",
        "\n",
        "r2_train_scores = {}\n",
        "r2_test_scores = {}\n",
        "\n",
        "# Train, predict, and evaluate each model\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    y_train_pred = model.predict(X_train_scaled)\n",
        "    y_test_pred = model.predict(X_test_scaled)\n",
        "    r2_train_scores[name] = model.r2_score(y_train, y_train_pred)\n",
        "    r2_test_scores[name] = model.r2_score(y_test, y_test_pred)\n",
        "    print(f\"R2 Score ({name}) - Train: {r2_train_scores[name]}, Test: {r2_test_scores[name]}\")\n",
        "\n",
        "# Define specific L1 ratios for grid search\n",
        "alphas = np.logspace(-4, 1, 10)  # Alpha values from 0.0001 to 10\n",
        "l1_ratios = [0.1, 0.3, 0.5, 0.7]  # Selected L1 ratio values\n",
        "\n",
        "# Perform grid search for ElasticNet\n",
        "best_model, best_params, best_score, elastic_net_results = grid_search_elastic_net(X_train_scaled, y_train, alphas, l1_ratios)\n",
        "\n",
        "# Print R2 scores for each alpha and l1_ratio with spacing for readability\n",
        "print(\"\\nElastic Net Scores for Different Alphas and L1 Ratios:\")\n",
        "last_alpha = None  # Track the last alpha value\n",
        "\n",
        "for alpha, l1_ratio, train_score, test_score in elastic_net_results:\n",
        "    # Print a newline if the alpha changes to improve readability\n",
        "    if last_alpha is not None and last_alpha != alpha:\n",
        "        print()  # Print a blank line to separate different alpha values\n",
        "\n",
        "    print(f\"Alpha: {alpha:.4f}, L1 Ratio: {l1_ratio}, Train R2: {train_score:.4f}, Test R2: {test_score:.4f}\")\n",
        "    last_alpha = alpha  # Update the last alpha value\n",
        "\n",
        "# Fit the best ElasticNet model on the training and test set\n",
        "best_model.fit(X_train_scaled, y_train)\n",
        "y_train_best_elastic = best_model.predict(X_train_scaled)\n",
        "y_test_best_elastic = best_model.predict(X_test_scaled)\n",
        "\n",
        "# Get R2 scores for the best ElasticNet model\n",
        "best_r2_train_score = best_model.r2_score(y_train, y_train_best_elastic)\n",
        "best_r2_test_score = best_model.r2_score(y_test, y_test_best_elastic)\n",
        "\n",
        "# Output best ElasticNet model results\n",
        "print(f\"\\nBest ElasticNet Model: alpha={best_params[0]}, l1_ratio={best_params[1]}, R2 Score - Train: {best_r2_train_score}, Test: {best_r2_test_score}\")\n",
        "\n",
        "# Plotting R2 scores for all models\n",
        "model_names = list(r2_train_scores.keys()) + ['Best ElasticNet']\n",
        "r2_train_scores_values = list(r2_train_scores.values()) + [best_r2_train_score]\n",
        "r2_test_scores_values = list(r2_test_scores.values()) + [best_r2_test_score]\n",
        "\n",
        "x = np.arange(len(model_names))  # the label locations\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plotting training R2 scores\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(x - 0.2, r2_train_scores_values, width=0.4, label='Training R² Score', color='blue')\n",
        "plt.axhline(0, color='grey', linestyle='--')\n",
        "plt.xticks(x, model_names, rotation=45)\n",
        "plt.xlabel('Regularization Type')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Training R² Scores for Different Models')\n",
        "plt.ylim(-1, 1)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "\n",
        "# Plotting testing R2 scores\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(x + 0.2, r2_test_scores_values, width=0.4, label='Testing R² Score', color='red')\n",
        "plt.axhline(0, color='grey', linestyle='--')\n",
        "plt.xticks(x, model_names, rotation=45)\n",
        "plt.xlabel('Regularization Type')\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('Testing R² Scores for Different Models')\n",
        "plt.ylim(-1, 1)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}